---
title: "Baybayin: Filipino Culture in Ink"
excerpt: "Multiclass classification of Handwritten Baybayin Characters <br/> <img src="/images/baybayin.jpg"> 
date: 2020-05-27
tags: [machine learning, data science, image processing]
header:
    image: "/images/baybayin.jpg"
    # excerpt: "Multiclass classification of Handwritten Baybayin Characters <img src="/images/baybayin.jpg"> "
---

## Multiclass classification of Handwritten Baybayin Characters

### Introduction

Baybayin is an ancient script that is making its way back to the Philippine culture. We see it in tattoo designs, museums, store signages, and perhaps more now that there is a bill passed saying that Baybayin be the national script. 

In this project, we aim to combine image processing and machine learning to see if we can train a machine to correctly identify the characters to their correct phonetics class.


## Highlights

Along this project, here are a couple of our highlights. 

1. There are hardly any online sources for character samples and people from AIM are really nice when asked for help.

2. When doing image processing, uniformity of formats is important. When the first resizing attempt was flawed, the learning accuracy was extremetly low at 20%. However, when we fixed that part of the script, it was easier on the machine.

3. Just like the study with the classification of digits from the MNIST dataset, it seems that SVM performs well for images in black and white. With it, roughly 83% was achieved. 

## Methodology

1. [Process Samples](#samples)
2. [Establish target classes](#targets)
3. [Create Data Array/Matrix](#matrix)
4. [Machine Learning with Balance Check](#ml)
5. [Validate Classification](#validate)
6. [Stacked Classifer](#stacked)
7. [Extra Sample Augmentation](#extra)
8. [Acknowledgements and References](#acknowledgement)



```python

```


```python
### libraries and functions

from PIL import Image, ImageOps
from skimage.feature import blob_dog, blob_log, blob_doh
from skimage.color import rgb2gray
import skimage.io as skio
import matplotlib.patches as patches
from sklearn.decomposition import PCA
# from skimage import data

import math
import os
from tqdm import tqdm_notebook as tqdm

#!/usr/bin/env python
# coding: utf-8

# In[4]:


from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge, Lasso
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import (RandomForestClassifier, 
                             GradientBoostingClassifier,
                             RandomForestRegressor,
                             GradientBoostingRegressor)

from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB

from functools import partial
from sklearn.model_selection import cross_val_score
from tqdm.autonotebook import tqdm

from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer as SI
from sklearn.model_selection  import GridSearchCV

import numpy as np
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from collections import defaultdict, Counter, OrderedDict
pd.options.display.float_format = '{:,.4g}'.format
import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


# # Functions
accuracy_collection = {}
params = {'n_neighbors': [1,3, 5, 7, 9, 11, 13, 15],
          'C': [1e-3, 1e-2, 1e-1, 1, 10, 100],
          'alpha': [1e-4, 1e-3, 1e-2,0.1, 1, 2, 5, 10],
          'none': [1],
          'max_depth': list(range(3, 5))#+[None], 
          
}

params_gs = {
    'KNClassifier':{'n_neighbors': [1,3, 5, 7, 9, 11, 13, 15]},
    'LogisticRegression L1': {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 500, 1000]},
    'LogisticRegression L2': {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 500, 1000]},
    'SVM L1': {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 500, 1000]},
    'SVM L2': {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 500, 1000]},
    'SVC RBF': {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 500, 1000], 
               'gamma': [1e-2,0.1, 1, 2, 3, 5, 10,
                        15, 20, 100, 500, 1000 ]},
    'SVC Poly': {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 500, 1000], 
                 'degree': [3],
                 'coef0': [0,5,10, 100]},
    'Random Forest': {'n_estimators': [100, 500, 1000], 
                     },
    'GBM': {'n_estimators': [100, 500, 1000], 
                      'max_features' :  [3, 5, 'auto'],
                      'learning_rate': np.arange(0.1, 3, 0.3).tolist(),
                      'max_depth' : list(range(3, 10, 2))#+[None],
                     },
    'Bayes_Multionomial': {'alpha': [1e-4, 1e-3, 1e-2,0.1, 1, 2, 5, 10]}
}


def impute_this(df, mean=[], median=[], most_frequent=[], fv=np.nan):
    
    if mean != []:
        for col_name in mean:
            imr = SI(strategy='mean', fill_value=fv)
            imputed = imr.fit_transform(df[[col_name]])
            df[col_name] = imputed
    if median != []:
        for col_name in mean:
            imr = SI(strategy='median', fill_value=fv)
            imputed = imr.fit_transform(df[[col_name]])
            df[col_name] = imputed
    if most_frequent != []:
        for col_name in mean:
            imr = SI(strategy='most_frequent', fill_value=fv)
            imputed = imr.fit_transform(df[[col_name]])
            df[col_name] = imputed
    return df
        


# ### Visualize Targets 

def plot_target_counts(y, regress=False):
    fig, ax = plt.subplots(figsize=(10,5))
        
    if not regress:
        sns.countplot(y, ax=ax);
        ax.set(title='Target Distribution')
        counts = y.value_counts()
        print(counts)
        pcc = np.sum((y.value_counts()/len(y))**2)
        print(f"1.25 * Proportional Chance Criterion {pcc*1.25}")
    else: 
        ax.plot(y.cumsum()/y.sum())
        ax.set(ylabel='%', xlabel='targets', title='Population Distribution')    
    
# ### Compute train test accuracys

def calc_accuracy_params(n_trials, key, X, y, model, setting_name, settings, 
                         hard_set=1, scaler=None):
    train_ = []
    test_ = []
    coefs_ = []
    global accuracy_collection 
    accuracy_collection[key] = {'train':[], 'test':[], setting_name:[], 'coefs':[]}
    with tqdm(total=len(settings)*n_trials) as pbar:
        for s in settings:
            train_acc = []
            test_acc = []
            best_acc = 0
            for n in range(n_trials):
                X_train, X_test,y_train, y_test = train_test_split(X, y, 
                                                            test_size=0.25,
                                                            random_state=n*8)
                if scaler: 
                    scaler.fit(X_train)
                    X_train = scaler.transform(X_train)
                    X_test = scaler.transform(X_test)
#                 clf = model(**{setting_name: s, 'n_jobs':-1})
                pca = PCA(n_components=70)
                pca.fit(X_train)
                X_train = pca.transform(X_train)
                X_test = pca.transform(X_test)
                try:
                    if setting_name != 'none':
                        clf = model(**{setting_name: s, 'n_jobs':-1})
                    else: 
                        clf = model()
                except TypeError:
                    clf = model(**{setting_name: s})
                clf.fit(X_train, y_train) 
                train_acc.append(clf.score(X_train, y_train))
                test_acc.append(clf.score(X_test, y_test))
                pbar.update(1)

            if np.mean(test_acc) > best_acc and hasattr(clf, 'coef_'):
                best_acc = np.mean(test_acc)
                coefs_ = clf.coef_
                accuracy_collection[key]['coefs'].append(coefs_)
            elif np.mean(test_acc) > best_acc and hasattr(clf, 'feature_importances_'):
                best_acc = np.mean(test_acc)
                coefs_ = clf.feature_importances_
                accuracy_collection[key]['coefs'].append(coefs_)
            accuracy_collection[key]['train'].append(train_acc)
            accuracy_collection[key]['test'].append(test_acc)
            accuracy_collection[key][setting_name].append([s]*n_trials)

            train_.append(np.mean(train_acc))
            test_.append(np.mean(test_acc))
    return (train_, test_, settings, coefs_, clf.predict(X_test))

def calculate_accuracy_params_gs(n_trials, model, key, X, y, scaler=None):
    with tqdm(total=4) as pbar:
        X_train, X_test,y_train, y_test = train_test_split(X, y, 
                                                        test_size=0.25, random_state=8)
        pbar.update(1)
        if scaler: 
            scaler.fit(X_train)
            X_train = scaler.transform(X_train)
            X_test = scaler.transform(X_test)
        pca = PCA(n_components=70)
        pca.fit(X_train)
        X_train = pca.transform(X_train)
        X_test = pca.transform(X_test)
        pbar.update(1)
        clf = GridSearchCV(model, param_grid=params_gs[key],
                                iid=False, cv=5,verbose=5,
                                n_jobs=-1)
        clf.fit(X_train, y_train)
        pbar.update(1)
        coefs_ = 'NA'
        clf_in = clf.best_estimator_
        if hasattr(clf_in, 'feature_importances_'):
            coefs_ = clf_in.feature_importances_
        pbar.update(1)
        
    return (clf_in.score(X_train, y_train), clf_in.score(X_test, y_test), 
            clf.best_params_, coefs_, clf_in.predict(X_test))

def we_classify(n_trials, X, y, features, scaler=None):    
    classify_models = {
        'KNClassifier': (KNeighborsClassifier, 'n_neighbors'),
        'LogisticRegression L1': (partial(LogisticRegression, 
                                          penalty='l1',
                                          solver='liblinear'), 
                                  'C'),
        'LogisticRegression L2': (partial(LogisticRegression, 
                                          penalty='l2',
                                          solver='liblinear'), 
                                  'C'),
        'SVM L1': (partial(LinearSVC, penalty='l1', loss='squared_hinge',
                               dual=False), 
                                  'C'),
        'SVM L2': (partial(LinearSVC, penalty='l2',loss='squared_hinge',
                               dual=False), 
                                  'C'),
        'Decision Tree': (DecisionTreeClassifier, 'max_depth'),
        'Bayes_Gaussian': (GaussianNB, 'none')
    }
    
    classify_gs_models = {
        'SVC Poly': SVC(kernel='poly', gamma='scale', max_iter=1000), 
        'SVC RBF': SVC(kernel='rbf', max_iter=1000), 
#         'Random Forest': RandomForestClassifier(), 
#         'GBM': GradientBoostingClassifier(), 
    }

    res = []
    for key, (model, setting) in classify_models.items():
        print(f'Running {key}')
        train, test, param, coefs, predict = calc_accuracy_params(n_trials, key, X, y, 
                                                         model, setting, 
                                                         params[setting],
                                                         scaler=scaler)
    
        ix = np.argmax(test)
        test_acc = test[ix]
        train_acc = train[ix]
        s = param[ix]
        ix_coefs = (np.unravel_index(np.argmax(coefs), coefs.shape)
                    [int(len(coefs.shape) == 2)]
                        if len(coefs) else None)
        predictor = features[ix_coefs] if ix_coefs else None
        res.append((key, train_acc, test_acc, {setting:s}, predictor, predict))
    
    #SVM Poly and Rbf, Ensemble
    for key, value in classify_gs_models.items():
        print(f'Running {key}')
        
        train, test, param, weights, predict = calculate_accuracy_params_gs(n_trials, value, key, X, y)
        
        if weights != 'NA':
            ix = np.argmax(weights)
            predictor = features[ix]
        else: 
            predictor = 'NA'
            
        res.append((key, train, test, param, predictor, predict))   

    return pd.DataFrame(res, columns=['model', 'train accurcy', 'test accuracy',
                                       'setting',
                                       'predictor', 'predict'])


# ### Visualize train test plot

def plot_accuracy(model_name, setting_name, flag=False):
    model_set = accuracy_collection[model_name]
    testdf = pd.DataFrame(zip(np.array(model_set['train']).flatten(), 
                               np.array(model_set['test']).flatten(), 
                               np.array(model_set[setting_name]).flatten()))
    train_ = testdf.groupby(2)[0].mean()
    test_ = testdf.groupby(2)[1].mean()
    fig, ax = plt.subplots(figsize=(12, 6))
    sns.lineplot(x=2, y=0, 
                      label="Training Accuracy", data=testdf,
                      ci='sd',
                      estimator='mean', markers=True, dashes=False, ax=ax);
    sns.lineplot(x=2, y=1,
                      label="Test Accuracy", data=testdf, 
                      ci='sd',
                      estimator='mean',
                      markers=True, dashes=False, ax=ax);
    title = f"""Accuracy vs {setting_name}
    Train Acc: {train_[test_.idxmax()]}   Test Acc: {test_[test_.idxmax()]}"""
    ax.set(xlabel=setting_name, ylabel='Accuracy', 
               title=title);
    if flag:
        ax.set_xscale('log')
        ax.set_xlabel(f"Log of {setting_name}")
    ax.axvline(test_.idxmax(), ls='--', c='gray')
    ax.legend();
 
```

    C:\Users\User\Anaconda3\lib\site-packages\tqdm\autonotebook\__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)
      " (e.g. in jupyter console)", TqdmExperimentalWarning)
    

### Process Samples <a id='samples'></a>

Each of the scanned samples will be subjected to image processing and cropped as described below.

1. Sets the image to black and white.
2. The numpy array of that black and white is fed to box_them_up function.
3. box_them_up scans the black and white for edge windows and merges them until the whole character is extracted.



```python
def box_them_up(image_gray):
    """Finds the bounding boxes for the blobs
    
    params: 
    -----------------------
    image_gray: numpy array of a black and white, inverted image where 
                background is black and foreground is white
    """
    def merge_overlap(a, b):
        """ Returns the maximum bounding box of a & b if these overlaps
        a, b
            ((min_x, min_y), (max_x, max_y))
        """
        amix = a[0][0]
        amiy = a[0][1]
        amax = a[1][0]
        amay = a[1][1]
        bmix = b[0][0]
        bmiy = b[0][1]
        bmax = b[1][0]
        bmay = b[1][1]
        if (bmix <= amix <= bmax or bmix <= amax <= bmax) and (bmiy <= amiy <= bmay or bmiy <= amay <= bmay):
            return ((min(amix, bmix), min(amiy, bmiy)), (max(amax, bmax), max(amay, bmay)))
        return None

    scan_size = 4
    blocks = []
    overlap_found = False
    # image_gray = image_gray[0:200, 600:]
    for r in range(0, image_gray.shape[0], scan_size // 2):
        for c in range(0, image_gray.shape[1], scan_size // 2):
            patch = image_gray[r:r+scan_size, c:c+scan_size]
            if np.sum(patch) > 0:
                for i in range(len(blocks)):
                    b = blocks[i]
                    rm = r + scan_size
                    cm = c + scan_size
                    ol = merge_overlap(((r, c), (rm, cm)), b)
                    if ol is not None:
                        blocks[i] = ol
                        overlap_found = True
                        break
                if overlap_found:
                    overlap_found = False
                    continue
                blocks.append(((r, c), (r + scan_size, c + scan_size)))

    has_overlap = True
    filter_boxes = list(set(blocks))
    while has_overlap:
        filter_boxes = list(set(filter_boxes))
        has_overlap = False
        for i in range(len(filter_boxes)):
            b = filter_boxes[i]
            for ii in range(i+1, len(filter_boxes)):
                bb = filter_boxes[ii]
                if b == bb:
                    continue
                ol = merge_overlap(bb, b) or merge_overlap(b, bb)
                if ol is not None:
                    has_overlap = True
                    filter_boxes[i] = ol
                    filter_boxes[ii] = ol

    plt.gcf().set_size_inches(10,10)
    ax = plt.gca()
    for b in filter_boxes:
        # draw in pyplot
        height =  b[0][1] - b[1][1]
        width = b[1][0] - b[0][0]
        rec = plt.Rectangle((b[0][1] - height, b[0][0]), height, width, linewidth=1, edgecolor='r', facecolor='none')

    return filter_boxes

```


```python
from_dir = 'scan'
to_dir = 'dataset2'

```


```python
# image = ImageOps.invert(Image.open('/Users/edavid/Downloads/final_004.tif').convert('L'))
# image = image.resize((image.size[0] // 4, image.size[1] // 4), Image.BICUBIC)
# image_gray = np.array(image)

# threshold = 150
# files = os.listdir('scan')
# with tqdm(total=len(files)) as pbar:
#     for f in files:

#         image = ImageOps.invert(Image.open(f'{from_dir}/{f}').rotate(270, expand=True).convert('L'))
#         image = image.resize((image.size[0] // 4, image.size[1] // 4), Image.BICUBIC)
#         image_gray = np.array(image)
#         image_gray[image_gray > threshold] = 255
#         image_gray[image_gray <= threshold] = 0
#         filter_boxes = box_them_up(image_gray)

#         for ix, b in enumerate(filter_boxes):
#     #     plt.matshow(Image.fromarray(image_gray[b[0][0]:b[1][0],b[0][1]:b[1][1]]), cmap='gray')
#         #                 .resize((28,28)))
#             im = Image.fromarray(image_gray[b[0][0]:b[1][0],b[0][1]:b[1][1]])
# #             im.thumbnail((28,28),Image.ANTIALIAS)
#             im.save(f"{to_dir}/{f}_{ix}.png")
#         pbar.update(1)
```

### Establish Target Classes <a id='targets'></a>

Listed below are the 17 classes based on the 17 phonetic characters of the ABAKADA song. 


```python
targets = [x for x in os.listdir(to_dir) if os.path.isdir(f'{to_dir}\{x}')]
print(f"Classes: {', '.join(targets)}")
```

    Classes: a, ba, dara, ei, ga, ha, ka, la, ma, na, nga, ou, pa, sa, ta, wa, ya
    

### Create Data Array/Matrix <a id='matrix'></a>

The function below serves to iterate over the directories, named just like the target classes, to further process the cropped images from above as follows:

1. The images are resized to 28x28 pixel form.
2. Each are then converted to a numpy array.
3. All the RGB designation lower than 100 are set to 0 and those higher are set to 1, thus creating a binary data matrix.
4. That matrix is reshaped to a 1x784 and concatenating into one resulting array.
5. That array is then converted to a dataframe.


```python
def image_to_df(to_dir, targets):
    """Takes every cropped image found in to_dir and returns a dataframe 
    with target column classified from the 'target' directory they were 
    found"""
    
    data = pd.DataFrame()
    for t in targets:
        print(f'Character {t}')
        files = os.listdir(f'{to_dir}\{t}')
        
        res_array = np.zeros((1, 784))
        with tqdm(total=len(files)) as pbar:
            for f in files:
                im0 = Image.open(f'{to_dir}\{t}\{f}').convert('L')
                im0 = im0.resize((28, 28), Image.BICUBIC)
                im_array0 = np.array(im0)
                im_array0 = np.where(im_array0 < 100, 0, im_array0)
                im_array0 = np.where(im_array0 >= 100, 1, im_array0)
                res_array = np.concatenate((res_array, im_array0.reshape((1,784))), axis=0)
                pbar.update(1)
        temp_df = pd.DataFrame(res_array)
        temp_df['target'] = t
        data = pd.concat((data, temp_df[1:]))
    return data
```


```python
#gets data frames and stores those records where > 0
data = image_to_df(to_dir, targets)
data = data[data.drop('target', axis=1).sum(axis=1) > 0]
```

    Character a
    


    HBox(children=(IntProgress(value=0, max=67), HTML(value='')))


    
    Character ba
    


    HBox(children=(IntProgress(value=0, max=69), HTML(value='')))


    
    Character dara
    


    HBox(children=(IntProgress(value=0, max=66), HTML(value='')))


    
    Character ei
    


    HBox(children=(IntProgress(value=0, max=57), HTML(value='')))


    
    Character ga
    


    HBox(children=(IntProgress(value=0, max=70), HTML(value='')))


    
    Character ha
    


    HBox(children=(IntProgress(value=0, max=62), HTML(value='')))


    
    Character ka
    


    HBox(children=(IntProgress(value=0, max=68), HTML(value='')))


    
    Character la
    


    HBox(children=(IntProgress(value=0, max=61), HTML(value='')))


    
    Character ma
    


    HBox(children=(IntProgress(value=0, max=61), HTML(value='')))


    
    Character na
    


    HBox(children=(IntProgress(value=0, max=69), HTML(value='')))


    
    Character nga
    


    HBox(children=(IntProgress(value=0, max=69), HTML(value='')))


    
    Character ou
    


    HBox(children=(IntProgress(value=0, max=67), HTML(value='')))


    
    Character pa
    


    HBox(children=(IntProgress(value=0, max=68), HTML(value='')))


    
    Character sa
    


    HBox(children=(IntProgress(value=0, max=66), HTML(value='')))


    
    Character ta
    


    HBox(children=(IntProgress(value=0, max=60), HTML(value='')))


    
    Character wa
    


    HBox(children=(IntProgress(value=0, max=62), HTML(value='')))


    
    Character ya
    


    HBox(children=(IntProgress(value=0, max=59), HTML(value='')))


    
    


```python
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 785 columns</p>
</div>




```python
data.shape
```




    (1095, 785)



Above is a head shot of 'data' and the X that will go towards training the machine with a sample of 1095 records/images.

### Machine Learning <a id='ml'></a>

Using the algorithms shown in the results, the datamatrix was split and used to train and test the machine to find the highest resulting accuracy.


```python
X = data.drop('target', axis=1).values
y = data['target'].values
```


```python
plot_target_counts(data['target'])
```

    ga      70
    ba      69
    na      69
    pa      68
    ka      68
    ou      67
    sa      66
    a       66
    dara    66
    nga     65
    wa      62
    ha      62
    ma      61
    la      60
    ta      60
    ya      59
    ei      57
    Name: target, dtype: int64
    1.25 * Proportional Chance Criterion 0.07380058797773191
    


![graph](/images/output_20_1.png)


The colorful graph above shows the distribution of samples collected. From our discretion, the samples are balanced and we continue with training and testing the machine.


```python
we_classify(50, X, y, data.drop('target', axis=1).columns )
```

    Running KNClassifier
    


    HBox(children=(IntProgress(value=0, max=400), HTML(value='')))


    
    Running LogisticRegression L1
    


    HBox(children=(IntProgress(value=0, max=300), HTML(value='')))

    
    Running LogisticRegression L2
    


    HBox(children=(IntProgress(value=0, max=300), HTML(value='')))

    
    Running SVM L1
    


    HBox(children=(IntProgress(value=0, max=300), HTML(value='')))


    
    Running SVM L2
    


    HBox(children=(IntProgress(value=0, max=300), HTML(value='')))


    
    Running Decision Tree
    


    HBox(children=(IntProgress(value=0), HTML(value='')))


    
    Running Bayes_Gaussian
    


    HBox(children=(IntProgress(value=0, max=50), HTML(value='')))


    
    Running SVC Poly
    


    HBox(children=(IntProgress(value=0, max=4), HTML(value='')))


    Fitting 5 folds for each of 32 candidates, totalling 160 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.9s
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.1s
    [Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:    6.1s finished
    

    
    Running SVC RBF
    


    HBox(children=(IntProgress(value=0, max=4), HTML(value='')))


    Fitting 5 folds for each of 96 candidates, totalling 480 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    2.2s
    [Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.8s
    [Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   11.0s
    [Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   17.7s
    [Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   19.7s finished
    

    
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>train accurcy</th>
      <th>test accuracy</th>
      <th>setting</th>
      <th>predictor</th>
      <th>predict</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNClassifier</td>
      <td>1</td>
      <td>0.7718</td>
      <td>{'n_neighbors': 1}</td>
      <td>None</td>
      <td>[ga, na, ga, sa, la, ba, ou, ma, ga, ga, ya, n...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LogisticRegression L1</td>
      <td>0.9371</td>
      <td>0.8256</td>
      <td>{'C': 1}</td>
      <td>40</td>
      <td>[ga, na, dara, sa, pa, ba, ou, ma, ta, ei, ya,...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>LogisticRegression L2</td>
      <td>0.9386</td>
      <td>0.8212</td>
      <td>{'C': 1}</td>
      <td>45</td>
      <td>[ga, na, ka, pa, pa, ba, ou, ma, ta, ga, ya, p...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SVM L1</td>
      <td>0.8943</td>
      <td>0.8205</td>
      <td>{'C': 0.1}</td>
      <td>45</td>
      <td>[ga, na, ya, pa, nga, ba, ou, ma, ta, ei, ya, ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SVM L2</td>
      <td>0.9371</td>
      <td>0.8221</td>
      <td>{'C': 0.1}</td>
      <td>53</td>
      <td>[ga, na, dara, la, nga, ba, ou, ma, ta, ga, la...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decision Tree</td>
      <td>0.523</td>
      <td>0.4477</td>
      <td>{'max_depth': 4}</td>
      <td>3</td>
      <td>[na, na, ka, ta, ka, na, ou, la, na, ka, ka, k...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Bayes_Gaussian</td>
      <td>0.8087</td>
      <td>0.7269</td>
      <td>{'none': 1}</td>
      <td>None</td>
      <td>[ga, wa, la, la, ya, ba, ou, ma, wa, ga, ya, w...</td>
    </tr>
    <tr>
      <th>7</th>
      <td>SVC Poly</td>
      <td>0.9525</td>
      <td>0.8066</td>
      <td>{'C': 0.1, 'coef0': 5, 'degree': 3}</td>
      <td>NA</td>
      <td>[dara, ha, na, ba, na, ya, ba, ga, ta, dara, n...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>SVC RBF</td>
      <td>0.9683</td>
      <td>0.8394</td>
      <td>{'C': 10, 'gamma': 0.01}</td>
      <td>NA</td>
      <td>[dara, ha, na, ba, na, ya, ba, ga, ya, ya, nga...</td>
    </tr>
  </tbody>
</table>
</div>




```python
we_classify(5, X, y, data.drop('target', axis=1).columns )
```

    Running KNClassifier
    


    HBox(children=(IntProgress(value=0, max=40), HTML(value='')))


    
    Running LogisticRegression L1
    


    HBox(children=(IntProgress(value=0, max=30), HTML(value='')))

    
    Running LogisticRegression L2
    


    HBox(children=(IntProgress(value=0, max=30), HTML(value='')))


    
    Running SVM L1
    


    HBox(children=(IntProgress(value=0, max=30), HTML(value='')))


    
    Running SVM L2
    


    HBox(children=(IntProgress(value=0, max=30), HTML(value='')))


    
    Running Decision Tree
    


    HBox(children=(IntProgress(value=0, max=10), HTML(value='')))


    
    Running Bayes_Gaussian
    


    HBox(children=(IntProgress(value=0, max=5), HTML(value='')))


    
    Running SVC Poly
    


    HBox(children=(IntProgress(value=0, max=4), HTML(value='')))


    Fitting 5 folds for each of 32 candidates, totalling 160 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.5s
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.9s
    [Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:    7.4s finished
    

    
    Running SVC RBF
    


    HBox(children=(IntProgress(value=0, max=4), HTML(value='')))


    Fitting 5 folds for each of 96 candidates, totalling 480 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    2.9s
    [Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    7.5s
    [Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   13.7s
    [Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   21.8s
    [Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   24.2s finished
    

    
    Running Random Forest
    


    HBox(children=(IntProgress(value=0, max=4), HTML(value='')))


    Fitting 5 folds for each of 3 candidates, totalling 15 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed:    1.0s remaining:    2.8s
    [Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    4.7s remaining:    4.1s
    [Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:    9.1s remaining:    2.2s
    [Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   11.2s finished
    

    
    Running GBM
    


    HBox(children=(IntProgress(value=0, max=4), HTML(value='')))


    Fitting 5 folds for each of 360 candidates, totalling 1800 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.1s
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.8min
    [Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.6min
    [Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  7.5min
    [Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 14.6min
    [Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 26.8min
    [Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed: 44.2min
    [Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed: 70.2min
    [Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 102.6min
    [Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 143.5min
    [Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed: 145.2min finished
    

    
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>train accurcy</th>
      <th>test accuracy</th>
      <th>setting</th>
      <th>predictor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNClassifier</td>
      <td>1</td>
      <td>0.762</td>
      <td>{'n_neighbors': 1}</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LogisticRegression L1</td>
      <td>0.9435</td>
      <td>0.8285</td>
      <td>{'C': 1}</td>
      <td>17</td>
    </tr>
    <tr>
      <th>2</th>
      <td>LogisticRegression L2</td>
      <td>0.9396</td>
      <td>0.8234</td>
      <td>{'C': 1}</td>
      <td>17</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SVM L1</td>
      <td>0.8955</td>
      <td>0.8292</td>
      <td>{'C': 0.1}</td>
      <td>17</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SVM L2</td>
      <td>0.935</td>
      <td>0.8226</td>
      <td>{'C': 0.1}</td>
      <td>17</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Decision Tree</td>
      <td>0.5118</td>
      <td>0.4372</td>
      <td>{'max_depth': 4}</td>
      <td>4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Bayes_Gaussian</td>
      <td>0.8085</td>
      <td>0.7314</td>
      <td>{'none': 1}</td>
      <td>None</td>
    </tr>
    <tr>
      <th>7</th>
      <td>SVC Poly</td>
      <td>0.9367</td>
      <td>0.7591</td>
      <td>{'C': 0.1, 'coef0': 5, 'degree': 3}</td>
      <td>NA</td>
    </tr>
    <tr>
      <th>8</th>
      <td>SVC RBF</td>
      <td>0.9562</td>
      <td>0.8029</td>
      <td>{'C': 10, 'gamma': 0.01}</td>
      <td>NA</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Random Forest</td>
      <td>1</td>
      <td>0.7847</td>
      <td>{'n_estimators': 500}</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>GBM</td>
      <td>1</td>
      <td>0.7701</td>
      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_fe...</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



Because SVM L1 shows to have the highest test accuracy with a better generalizaion, we run with it and get an accuracy as seen below and a corresponding confusion matrix.


```python
test_acc=[]

X_train, X_test,y_train, y_test = train_test_split(X, y, 
                                            test_size=0.25,
                                            random_state=8)

pca = PCA(n_components=70)
pca.fit(X_train)
X_train = pca.transform(X_train)
X_test = pca.transform(X_test)
clf = LinearSVC(penalty='l1', loss='squared_hinge',dual=False,C=0.1)


clf.fit(X_train, y_train) 
print(clf.score(X_test, y_test))

```

    0.8394160583941606
    


```python
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
y_pred = clf.predict(X_test)
confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)

fig, ax = plt.subplots(figsize=(10, 10))
ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)
for i in range(confmat.shape[0]):
    for j in range(confmat.shape[1]):
        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')
plt.xlabel('predicted label')
plt.ylabel('true label')
plt.show()
print(classification_report(y_test, y_pred))
```


![graph](images/output_26_0.png)


                  precision    recall  f1-score   support
    
               a       0.82      0.90      0.86        20
              ba       1.00      1.00      1.00        21
            dara       0.89      0.67      0.76        12
              ei       1.00      0.93      0.97        15
              ga       0.85      0.85      0.85        20
              ha       0.88      0.82      0.85        17
              ka       0.80      0.86      0.83        14
              la       0.65      0.94      0.77        16
              ma       0.92      0.65      0.76        17
              na       0.88      0.93      0.90        15
             nga       0.70      0.88      0.78        16
              ou       0.94      1.00      0.97        16
              pa       0.67      0.62      0.64        13
              sa       1.00      0.80      0.89        20
              ta       0.92      0.71      0.80        17
              wa       0.84      0.89      0.86        18
              ya       0.44      0.57      0.50         7
    
       micro avg       0.84      0.84      0.84       274
       macro avg       0.83      0.82      0.82       274
    weighted avg       0.85      0.84      0.84       274
    
    

### Validate Classification<a id='validate'></a>

Naturally, we went and validated the test.


```python
validate_data = image_to_df('validateset\set', targets)
validate_data = validate_data[validate_data.drop('target', axis=1).sum(axis=1) > 1]
```

    Character a
    


    HBox(children=(IntProgress(value=0, max=24), HTML(value='')))


    
    Character ba
    


    HBox(children=(IntProgress(value=0, max=22), HTML(value='')))


    
    Character dara
    


    HBox(children=(IntProgress(value=0, max=24), HTML(value='')))


    
    Character ei
    


    HBox(children=(IntProgress(value=0, max=13), HTML(value='')))


    
    Character ga
    


    HBox(children=(IntProgress(value=0, max=22), HTML(value='')))


    
    Character ha
    


    HBox(children=(IntProgress(value=0, max=23), HTML(value='')))


    
    Character ka
    


    HBox(children=(IntProgress(value=0, max=23), HTML(value='')))


    
    Character la
    


    HBox(children=(IntProgress(value=0, max=20), HTML(value='')))


    
    Character ma
    


    HBox(children=(IntProgress(value=0, max=19), HTML(value='')))


    
    Character na
    


    HBox(children=(IntProgress(value=0, max=22), HTML(value='')))


    
    Character nga
    


    HBox(children=(IntProgress(value=0, max=22), HTML(value='')))


    
    Character ou
    


    HBox(children=(IntProgress(value=0, max=23), HTML(value='')))


    
    Character pa
    


    HBox(children=(IntProgress(value=0, max=23), HTML(value='')))


    
    Character sa
    


    HBox(children=(IntProgress(value=0, max=24), HTML(value='')))


    
    Character ta
    


    HBox(children=(IntProgress(value=0, max=20), HTML(value='')))


    
    Character wa
    


    HBox(children=(IntProgress(value=0, max=19), HTML(value='')))


    
    Character ya
    


    HBox(children=(IntProgress(value=0, max=17), HTML(value='')))


    
    


```python
validate_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>a</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 785 columns</p>
</div>




```python
XX = validate_data.drop('target', axis=1).values
yy = validate_data['target'].values
```


```python
XX.shape
```




    (348, 784)




```python
X_train, X_test,y_train, y_test = train_test_split(X, y, stratify=y, 
                                                        test_size=0.25)

pca = PCA(n_components=70)
pca.fit(X_train)
X_train = pca.transform(X_train)
X_validate = pca.transform(XX)
clf = partial(LinearSVC, penalty='l1', loss='squared_hinge',
                               dual=False)(**{'C':0.1})
clf.fit(X_train, y_train)

coefs_ = 'NA'

```

With a sample sie of 348, we got a validation score of 71.6% using the same SVM. 


```python
clf.score(X_validate, yy)
```




    0.7155172413793104



### Stacked Classifier <a id='stacked'></a>

In hopes of increasing accuracy, we had also attempted to increase accuracy by attempting to stack classifiers. 


```python
data_ = pd.concat((data, validate_data))
XXX = np.concatenate((X, XX), axis=0)
yyy = np.concatenate((y, yy), axis=0)
```


```python
classify_gs_models = {
        'KNClassifier': KNeighborsClassifier(),
        'LogisticRegression L1': LogisticRegression(
                                          penalty='l1',
                                          solver='liblinear'),
        'LogisticRegression L2': LogisticRegression(
                                          penalty='l2',
                                          solver='liblinear'),
        'SVM L1': LinearSVC( penalty='l1', loss='squared_hinge',
                               dual=False), 
        'SVM L2': LinearSVC( penalty='l2',loss='squared_hinge',
                               dual=False), 
        'SVC Poly': SVC(kernel='poly', gamma='scale', max_iter=1000), 
        'SVC RBF': SVC(kernel='rbf', max_iter=1000), 
    }
    
    
```


```python

X_train, X_test,y_train, y_test = train_test_split(X, y,
                                            test_size=0.25,
                                            random_state=8)
predicts=[]
with tqdm(total=8) as pbar:
    for key, model in classify_gs_models.items():
        print(f"Processing {key}")
        pca = PCA(n_components=70)
        pca.fit(X_train)
        X_train = pca.transform(X_train)
        X_test = pca.transform(X_test)
        clf = GridSearchCV(model, param_grid=params_gs[key],
                                iid=False, cv=5,verbose=5,
                                n_jobs=-1)
        clf.fit(X_train, y_train)
        coefs_ = 'NA'
        clf_in = clf.best_estimator_
        predicts.append((key, clf_in.predict(X_test)))
        pbar.update(1)

```


    HBox(children=(IntProgress(value=0, max=8), HTML(value='')))


    Processing KNClassifier
    Fitting 5 folds for each of 8 candidates, totalling 40 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.5s
    [Parallel(n_jobs=-1)]: Done  34 out of  40 | elapsed:    4.3s remaining:    0.7s
    [Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    4.4s finished
    

    Processing LogisticRegression L1
    Fitting 5 folds for each of 8 candidates, totalling 40 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s
    [Parallel(n_jobs=-1)]: Done  16 out of  40 | elapsed:    0.6s remaining:    1.0s
    [Parallel(n_jobs=-1)]: Done  34 out of  40 | elapsed:   17.6s remaining:    3.0s
    [Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   22.0s finished
    

    Processing LogisticRegression L2
    Fitting 5 folds for each of 8 candidates, totalling 40 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s
    [Parallel(n_jobs=-1)]: Done  16 out of  40 | elapsed:    0.5s remaining:    0.8s
    [Parallel(n_jobs=-1)]: Done  34 out of  40 | elapsed:    2.5s remaining:    0.4s
    [Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    3.1s finished
    

    Processing SVM L1
    Fitting 5 folds for each of 8 candidates, totalling 40 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s
    [Parallel(n_jobs=-1)]: Done  16 out of  40 | elapsed:    4.5s remaining:    6.8s
    [Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   26.1s finished
    

    Processing SVM L2
    Fitting 5 folds for each of 8 candidates, totalling 40 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s
    [Parallel(n_jobs=-1)]: Done  16 out of  40 | elapsed:    0.6s remaining:    1.0s
    [Parallel(n_jobs=-1)]: Done  34 out of  40 | elapsed:    9.8s remaining:    1.6s
    [Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   12.3s finished
    

    Processing SVC Poly
    Fitting 5 folds for each of 32 candidates, totalling 160 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.3s
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    1.9s
    [Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:    4.6s finished
    

    Processing SVC RBF
    Fitting 5 folds for each of 96 candidates, totalling 480 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.3s
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    3.1s
    [Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    8.3s
    [Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   15.5s
    [Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   25.0s
    [Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   27.9s finished
    

    
    


```python
sf = pd.DataFrame(predicts).set_index(0)
sf[2] = sf[1].apply(lambda x: ','.join(x))
sf.drop(1, axis=1, inplace=True)
sf_2 = sf[2].str.split(',', expand=True).T
sf_2['target'] = y_test

```


```python
target_dict = {c: i+1 for i,c in enumerate(sorted(targets))}

for c in sf_2.columns:
    sf_2.loc[:, c] = sf_2.loc[:, c].apply(lambda x: target_dict[x])
sf_2.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>KNClassifier</th>
      <th>LogisticRegression L1</th>
      <th>LogisticRegression L2</th>
      <th>SVM L1</th>
      <th>SVM L2</th>
      <th>SVC Poly</th>
      <th>SVC RBF</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>15</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13</td>
      <td>14</td>
      <td>14</td>
      <td>14</td>
      <td>14</td>
      <td>6</td>
      <td>6</td>
      <td>14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>



Seen above is the predicted outcome of each of the other classifiers which now becomes the dataset which we will feed into XGBoost. 


```python
from xgboost import XGBClassifier
from sklearn.metrics import make_scorer, accuracy_score
Xs = sf_2.drop('target', axis=1).values
ys = sf_2['target'].values
```


```python
Xs_train, Xs_test,ys_train, ys_test = train_test_split(Xs, ys,
                                            test_size=0.25,
                                            random_state=8)

xgb = XGBClassifier()
xgb_parameters = {'learning_rate': [0.1, 0.2], 
              'max_depth': [3, 5, ],
              'min_child_weight': [0, 1],
              'colsample_bytree': [0.8, 0.9],
              'subsample': [0.8, 0.9]}
clf_xgb = GridSearchCV(xgb, xgb_parameters, n_jobs=-1, 
                   cv=5, 
                   scoring= make_scorer(accuracy_score),
                   verbose=2, refit=True)
clf_xgb.fit(Xs_train, ys_train)

xgb_test = XGBClassifier(max_depth=clf_xgb.best_params_['max_depth'],
                         learning_rate=clf_xgb.best_params_['learning_rate'],
                         min_child_weight=clf_xgb.best_params_['min_child_weight'],
                         colsample_bytree=clf_xgb.best_params_['colsample_bytree'],
                         subsample=clf_xgb.best_params_['subsample'])
xgb_fit = xgb_test.fit(Xs_train, ys_train)
print(accuracy_score(ys_test, xgb_fit.predict(Xs_test)))

```

    Fitting 5 folds for each of 32 candidates, totalling 160 fits
    

    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
    [Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.4s
    [Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:   21.5s finished
    C:\Users\User\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)
    

    0.8260869565217391
    

As you can see, the results are almost the same as that with the SVM at 82.6%. 

### Extra Samples via Augmentation <a id='extra'></a>

We also attempted to increase our sample set to about 6000 per Baybayin character via augmentations learned in MDS in term 1.

```python
def random_rotation(image_array: ndarray):
    # pick a random degree of rotation between 15% on the left and 15% on the right
    random_degree = random.uniform(-15, 15)
    return sk.transform.rotate(image_array, random_degree)

def random_warp(image_array: ndarray):
    x = random.uniform(-3, 3)
    y = random.uniform(-3, 3)
    matrix = np.array([[1, 0, x], [0, 1, y], [0, 0, 1]])
    return sk.transform.warp(image_array, matrix)

# apply rotation and warp
path1 = []
for d in dirs:
    for file in os.listdir(d):
        fpath = os.path.join(d, file)

        img = Image.open(fpath).convert('L')
        im0 = img.resize((28, 28), Image.BICUBIC)
        array = np.asarray(im0)    
        for i in range(5):
            fname = f'rotate-{i}-{file}'
            new_img = random_rotation(array)
            new_path = os.path.join(d, fname)
            sk.io.imsave(new_path, new_img)

        for i in range(5):
            fname = f'warp-{i}-{file}'
            new_img = random_warp(array)
            new_path = os.path.join(d, fname)
            sk.io.imsave(new_path, new_img)
```


```python
data_aug = image_to_df('augmentset\dataset2 - augmented', targets)
```

    Character a
    


    HBox(children=(IntProgress(value=0, max=7437), HTML(value='')))


    
    Character ba
    


    HBox(children=(IntProgress(value=0, max=7659), HTML(value='')))


    
    Character dara
    


    HBox(children=(IntProgress(value=0, max=7326), HTML(value='')))


    
    Character ei
    


    HBox(children=(IntProgress(value=0, max=6327), HTML(value='')))


    
    Character ga
    


    HBox(children=(IntProgress(value=0, max=7770), HTML(value='')))


    
    Character ha
    


    HBox(children=(IntProgress(value=0, max=6882), HTML(value='')))


    
    Character ka
    


    HBox(children=(IntProgress(value=0, max=7548), HTML(value='')))


    
    Character la
    


    HBox(children=(IntProgress(value=0, max=6771), HTML(value='')))


    
    Character ma
    


    HBox(children=(IntProgress(value=0, max=6771), HTML(value='')))


    
    Character na
    


    HBox(children=(IntProgress(value=0, max=7659), HTML(value='')))


    
    Character nga
    


    HBox(children=(IntProgress(value=0, max=7659), HTML(value='')))


    
    Character ou
    


    HBox(children=(IntProgress(value=0, max=7437), HTML(value='')))


    
    Character pa
    


    HBox(children=(IntProgress(value=0, max=7548), HTML(value='')))


    
    Character sa
    


    HBox(children=(IntProgress(value=0, max=7326), HTML(value='')))


    
    Character ta
    


    HBox(children=(IntProgress(value=0, max=6660), HTML(value='')))


    
    Character wa
    


    HBox(children=(IntProgress(value=0, max=6882), HTML(value='')))


    
    Character ya
    


    HBox(children=(IntProgress(value=0, max=6549), HTML(value='')))


    
    


```python
Xa = data_aug.drop('target', axis=1).values
ya = data_aug['target'].values
```


```python
data_aug.shape
X_a, X_validate, y_a, y_validate = train_test_split(Xa, ya,
                                            test_size=0.25,
                                            random_state=8)
```


```python
test_acc=[]

Xa_train, Xa_test,ya_train, ya_test = train_test_split(X_a, y_a,
                                            test_size=0.25,
                                            random_state=8)

pca = PCA(n_components=70)
pca.fit(Xa_train)
X_train = pca.transform(Xa_train)
X_test = pca.transform(Xa_test)
clf = LinearSVC(penalty='l1', loss='squared_hinge',dual=False,C=0.1)


clf.fit(Xa_train, ya_train) 
print(clf.score(Xa_test, ya_test))
print(clf.score(X_validate, y_validate))

```

    0.795025092734017
    0.791509835368049
    

The above numbers are the test and validation accuracies of SVM. At roughly 79%, we can see that increasing the sample size does not affect our results much. This means that our earlier samples set of 1095 is sufficient for testing. 

## Acknowledgements <a id='acknowledgement'></a>

1. LT team mates
2. Eduardo David, Jr
3. MSDS 2019 and MSIB 2020
4. Professor Christopher Monterola

<img src="/images/baybayin_team.jpg" width="800" />

## References
https://medium.com/the-andela-way/applying-machine-learning-to-recognize-handwritten-characters-babcd4b8d705  
https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_blob.html  
https://www.pythonforbeginners.com/gui/how-to-use-pillow  
https://pillow.readthedocs.io/en/3.1.x/reference/Image.html


